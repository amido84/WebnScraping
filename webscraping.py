# -*- coding: utf-8 -*-
"""Webscraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13cbCP2kQrnJWTtqCHphHro85SdoSyJWg

# Web scraping for Data Scientist job in CO
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
import pandas as pd
import warnings

df = pd.DataFrame(columns=['job_title', 'company', 'location', 'job_description','Python','SQL','AWS','RESTFUL','Machine learning','Deep Learning','Text Mining','NLP','SAS','Tableau','Sagemaker','TensorFlow','Spark'])

# open a web browser
driver = webdriver.Chrome()

for page_num in range(0, 10):
    # navigate to the page
    driver.get(f'https://www.indeed.com/jobs?q=data+scientist&l=CO&start={page_num*10}')

    # extract the information for each job listing on the page
    job_titles = driver.find_elements(By.XPATH, '//h2[@class="jobTitle css-1h4a4n5 eu4oa1w0"]/a')
    companies = driver.find_elements(By.XPATH, '//span[@class="companyName"]')
    locations = driver.find_elements(By.XPATH, "//div[contains(@class, 'companyLocation')]")
    job_descriptions = driver.find_elements(By.XPATH, "//div[contains(@class, 'job-snippet')]")


    # iterate through the list of job listings
    for i in range(min(len(job_titles), len(companies), len(locations), len(job_descriptions))):
        # extract the text for each element
        job_title = job_titles[i].text
        company = companies[i].text
        location = locations[i].text
        job_description = job_descriptions[i].text

    # extract the job IDs for each listing on the page
    job_ids = driver.find_elements(By.XPATH, '//h2[@class="jobTitle css-1h4a4n5 eu4oa1w0"]/a')

    for i in range(len(job_ids)):
        j_id = job_ids[i].get_attribute('data-jk')


        # extract the skills information from the job page
        skills = driver.find_elements(By.XPATH, "//div[contains(@class, 'jobsearch-jobDescriptionText')]")
        for i in range(len(skills)):
            skills = skills[i].text


        # create a dictionary with the extracted information
        job_info = {'job_title': job_title, 'company': company, 'location': location, 'job_description': job_description, 'Python':False,'SQL':False,'AWS':False,'RESTFUL':False,'Machine learning':False,'Deep Learning':False,'Text Mining':False,'NLP':False,'SAS':False,'Tableau':False,'Sagemaker':False,'TensorFlow':False,'Spark':False}
        for keyword in ['Python','SQL','AWS','RESTFUL','Machine learning','Deep Learning','Text Mining','NLP','SAS','Tableau','Sagemaker','TensorFlow','Spark']:
            if keyword in skills:
                job_info[keyword] = True
        # add the dictionary to the DataFrame
        df = pd.concat([df, pd.DataFrame([job_info])], ignore_index=True)

warnings.filterwarnings("ignore", message="In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True")
df = pd.concat([df, pd.DataFrame([job_info])], ignore_index=True)

# close the web browser
driver.quit()


html_table = df.to_html()
with open('table.html', 'w') as f:
    f.write(html_table)
from IPython.display import display, HTML
display(HTML(df.to_html()))

"""# Save you DataFrame to a pickle file name *indeed_job_co.pkl*.

   <font color='red'>upload the pickle file(indeed_job_co.pkl) along with solution notebook to the canvas</font>
"""

# Save DataFrame to pickle file
df.to_pickle("indeed_job_co.pkl")

import pandas as pd
df = pd.read_pickle('indeed_job_co.pkl')
df.head()

import pickle
with open('indeed_job_co.pkl', 'rb') as f:
    job_df = pickle.loads(f.read())
job_df.head()

"""
# Which city has maximum job posting.

"""

max_city = df['location'].value_counts().idxmax()
print(f"The city with the most job postings is {max_city}.")

"""# Top 3 most demanding skills(like Python, AWS, SQL ...)


"""

skills = ['Python', 'SQL', 'AWS', 'RESTFUL', 'Machine_Learning', 'Deep_Learning', 'Text_Mining', 'NLP',
          'SAS', 'Tableau', 'Sagemaker', 'TensorFlow', 'Spark']

# Ensure column names match with the skills
column_names = job_df.columns
matching_skills = [skill.replace('_', ' ') for skill in skills if skill in column_names]
top_skills = job_df[matching_skills].sum().sort_values(ascending=False).head(3)
print(f"The top 3 most demanding skills are:\n{top_skills}.")

